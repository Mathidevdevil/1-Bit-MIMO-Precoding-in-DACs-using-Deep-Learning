{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["##Drive Connection"],"metadata":{"id":"AdNPnV34eCk0"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"6oazyZ8Vs_v1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1748320367885,"user_tz":-330,"elapsed":23595,"user":{"displayName":"Mathi","userId":"02184997273667573842"}},"outputId":"06f60832-7bae-4706-fc96-304f476d50b6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##Importing Dataset"],"metadata":{"id":"ZFObiQVkjXCc"}},{"cell_type":"code","source":["import zipfile\n","import os\n","zip_file_path = '/content/drive/MyDrive/Final_Year_Project.zip'\n","extract_path = '/content/Final_Year_Project'\n","\n","with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n","    zip_ref.extractall(extract_path)\n","\n","print(\"Contents extracted to:\", extract_path)\n","print(os.listdir(extract_path))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xctB1QTqCPdO","outputId":"5f5fcd8d-a357-49d7-be36-f414a3a6a63a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Contents extracted to: /content/Final_Year_Project\n","['Final_Year_Project']\n"]}]},{"cell_type":"markdown","source":["##Installing Dependencies"],"metadata":{"id":"ZqDPrNxCieYH"}},{"cell_type":"code","source":["!pip install thop\n","!pip uninstall torch torchvision torchaudio -y\n","!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8_VN54_sn1ep","outputId":"4aac7faf-5a83-4071-b6d1-c5b8391ab612","executionInfo":{"status":"ok","timestamp":1746869950709,"user_tz":-330,"elapsed":299382,"user":{"displayName":"Mathi","userId":"02184997273667573842"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting thop\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.6.0+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->thop)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->thop)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->thop)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->thop)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->thop)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->thop)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->thop)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->thop)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->thop)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->thop)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\n","Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238\n","Found existing installation: torch 2.6.0+cu124\n","Uninstalling torch-2.6.0+cu124:\n","  Successfully uninstalled torch-2.6.0+cu124\n","Found existing installation: torchvision 0.21.0+cu124\n","Uninstalling torchvision-0.21.0+cu124:\n","  Successfully uninstalled torchvision-0.21.0+cu124\n","Found existing installation: torchaudio 2.6.0+cu124\n","Uninstalling torchaudio-2.6.0+cu124:\n","  Successfully uninstalled torchaudio-2.6.0+cu124\n","Looking in indexes: https://download.pytorch.org/whl/cu118\n","Collecting torch\n","  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n","Collecting torchvision\n","  Downloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n","Collecting torchaudio\n","  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Collecting sympy>=1.13.3 (from torch)\n","  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n","  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting triton==3.3.0 (from torch)\n","  Downloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.0->torch) (75.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading https://download.pytorch.org/whl/cu118/torch-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (955.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m955.6/955.6 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/triton-3.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (156.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchvision-0.22.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (6.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/cu118/torchaudio-2.7.0%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl (3.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl (6.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch, torchvision, torchaudio\n","  Attempting uninstall: triton\n","    Found existing installation: triton 3.2.0\n","    Uninstalling triton-3.2.0:\n","      Successfully uninstalled triton-3.2.0\n","  Attempting uninstall: sympy\n","    Found existing installation: sympy 1.13.1\n","    Uninstalling sympy-1.13.1:\n","      Successfully uninstalled sympy-1.13.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 sympy-1.13.3 torch-2.7.0+cu118 torchaudio-2.7.0+cu118 torchvision-0.22.0+cu118 triton-3.3.0\n"]}]},{"cell_type":"markdown","source":["##Dataset Generation\n","\n"],"metadata":{"id":"Fw6QEV1Nlw4B"}},{"cell_type":"code","source":["import numpy as np\n","from scipy.linalg import toeplitz, pinv, sqrtm\n","from scipy.special import jv\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","import h5py\n","import yaml\n","import torch\n","from torch.utils.data import Dataset\n","from typing import Dict, List, Optional\n","import pandas as pd\n","import os\n","\n","# Set device to GPU if available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device: {device}\")\n","\n","try:\n","    import qd_realization\n","except ImportError:\n","    print(\"Warning: qd_realization not installed - using enhanced fallback channel model\")\n","\n","# ======================== Configuration ========================\n","class Config:\n","    # System Parameters\n","    NUM_BS_ANTENNAS = 32  # Changed to 32 antennas\n","    NUM_UE = 4           # Changed to 4 users\n","    NUM_SUBCARRIERS = 64\n","    CP_LENGTH = 16\n","    PILOT_RATIO = 0.1\n","\n","    # Simulation Parameters\n","    NUM_SAMPLES = 120000\n","    SNR_DB_RANGE = np.linspace(0, 20, 5)  # SNR range from 0 to 20 dB\n","    DOPPLER_RANGE = [1, 5, 20, 100]\n","    VELOCITY_RANGE = [3, 60]\n","\n","    # Hardware Impairments\n","    PHASE_NOISE_DEG = 5\n","    IQ_IMBALANCE = 0.03\n","    DAC_QUANTIZATION_BITS = 1\n","    DAC_NOISE_VAR = 0.01\n","    PA_NONLINEARITY_ALPHA = 2.0\n","\n","    # 3GPP Channel\n","    SCENARIO = 'UMi'\n","    CARRIER_FREQ = 2.4e9\n","    POLARIZATION = 'dual'\n","\n","# ======================== 1-Bit Quantization ========================\n","def quantize_1bit(x: np.ndarray) -> np.ndarray:\n","    return (np.sign(np.real(x)) + 1j * np.sign(np.imag(x))) / np.sqrt(2)\n","\n","def apply_dac_effects(x: np.ndarray) -> np.ndarray:\n","    x_quantized = quantize_1bit(x)\n","    noise = np.sqrt(Config.DAC_NOISE_VAR/2) * (\n","        np.random.randn(*x.shape) + 1j*np.random.randn(*x.shape))\n","    return x_quantized + noise\n","\n","# ======================== Enhanced 3GPP Channel Model ========================\n","class ThreeGPPChannel:\n","    def __init__(self):\n","        self.channel = None\n","        if 'qd_realization' in globals():\n","            self.channel = qd_realization.TDL(Config.SCENARIO, Config.CARRIER_FREQ)\n","\n","    def generate(self, t: float, velocity: float) -> np.ndarray:\n","        if self.channel:\n","            H = self.channel.generate(Config.NUM_BS_ANTENNAS, Config.NUM_UE, velocity)\n","        else:\n","            H = self._generate_enhanced_fallback(t, velocity)\n","\n","        if Config.POLARIZATION == 'dual':\n","            H = np.stack([H, 0.3*H + np.random.randn(*H.shape)*0.1], axis=-1)\n","        return H\n","\n","    def _generate_enhanced_fallback(self, t: float, velocity: float) -> np.ndarray:\n","        num_taps = 8\n","        delays = np.sort(np.random.uniform(0, 300e-9, num_taps))\n","        velocity = float(velocity)\n","        doppler = velocity * Config.CARRIER_FREQ / 3e8\n","        time_phase = 2 * np.pi * doppler * t * np.random.uniform(-1, 1, num_taps)\n","\n","        gains = np.exp(-delays/100e-9 + 1j*time_phase)[:,None,None] * (\n","            np.random.randn(num_taps, Config.NUM_UE, Config.NUM_BS_ANTENNAS) +\n","            1j*np.random.randn(num_taps, Config.NUM_UE, Config.NUM_BS_ANTENNAS))\n","\n","        H_freq = np.zeros((Config.NUM_SUBCARRIERS, Config.NUM_UE, Config.NUM_BS_ANTENNAS), dtype=complex)\n","        for sc in range(Config.NUM_SUBCARRIERS):\n","            phase = -2*np.pi*sc/Config.NUM_SUBCARRIERS * delays\n","            H_freq[sc] = np.sum(gains * np.exp(1j*phase[:,None,None]), axis=0)\n","\n","        corr = 0.7\n","        R = toeplitz(corr**np.arange(Config.NUM_BS_ANTENNAS))\n","        return H_freq @ sqrtm(R)\n","\n","# ======================== Advanced 1-Bit Precoding ========================\n","def mmse_1bit_precoder(H: np.ndarray, s: np.ndarray) -> np.ndarray:\n","    \"\"\"Fixed MMSE precoder with robust dimension handling\"\"\"\n","    # Ensure H has shape (num_ue, num_bs_antennas)\n","    if H.shape[0] == Config.NUM_BS_ANTENNAS and H.shape[1] == Config.NUM_UE:\n","        H = H.T  # Transpose if dimensions are swapped\n","\n","    # Verify final dimensions\n","    assert H.shape == (Config.NUM_UE, Config.NUM_BS_ANTENNAS), \\\n","        f\"Channel matrix must be (num_ue, num_bs), got {H.shape}\"\n","    assert s.shape == (Config.NUM_UE,), \\\n","        f\"Symbol vector must be (num_ue,), got {s.shape}\"\n","\n","    # Regularized pseudo-inverse with proper dimensions\n","    regularization = 1e-3 * np.eye(Config.NUM_BS_ANTENNAS)\n","    W = pinv(H.conj().T @ H + regularization) @ H.conj().T @ s.reshape(-1, 1)\n","\n","    # Quantization and power normalization\n","    W_1bit = quantize_1bit(W.flatten())\n","    return W_1bit / np.sqrt(np.mean(np.abs(W_1bit)**2))  # Unit power normalization\n","\n","def apply_pa_nonlinearity(x: np.ndarray) -> np.ndarray:\n","    x_abs = np.abs(x)\n","    return x * (1 - np.exp(-x_abs**Config.PA_NONLINEARITY_ALPHA)) / (x_abs + 1e-6)\n","\n","# ======================== OFDM Processing ========================\n","class OFDM:\n","    @staticmethod\n","    def modulate(x_freq: np.ndarray) -> np.ndarray:\n","        x_time = np.fft.ifft(x_freq, axis=0)\n","        x_time_cp = np.concatenate([x_time[-Config.CP_LENGTH:], x_time], axis=0)\n","        x_dac = apply_dac_effects(x_time_cp)\n","        return apply_pa_nonlinearity(x_dac)\n","\n","    @staticmethod\n","    def demodulate(y_time: np.ndarray) -> np.ndarray:\n","        y_data = y_time[Config.CP_LENGTH:Config.CP_LENGTH+Config.NUM_SUBCARRIERS]\n","        return np.fft.fft(y_data, axis=0)\n","\n","# ======================== Dataset Generation ========================\n","class MassiveMIMO1BitDataset(Dataset):\n","    def __init__(self, file_path: str):\n","        with h5py.File(file_path, 'r') as f:\n","            self.channels = torch.tensor(np.array(f['channels']), dtype=torch.complex64).to(device)\n","            self.precoders = torch.tensor(np.array(f['precoders_1bit']), dtype=torch.complex64).to(device)\n","            self.received_signals = torch.tensor(np.array(f['received_signals']), dtype=torch.complex64).to(device)\n","\n","    def __len__(self):\n","        return len(self.channels)\n","\n","    def __getitem__(self, idx):\n","        return {\n","            'channel': self.channels[idx],\n","            'precoder': self.precoders[idx],\n","            'received_signal': self.received_signals[idx]\n","        }\n","\n","def generate_symbols() -> np.ndarray:\n","    return (2*np.random.randint(0, 2, (Config.NUM_UE,)) - 1) + \\\n","           1j*(2*np.random.randint(0, 2, (Config.NUM_UE,)) - 1)\n","\n","def save_to_csv(results: dict, filename: str):\n","    data = []\n","\n","    for i in tqdm(range(len(results['transmitted_symbols'])), desc=\"Saving to CSV\"):\n","        row = {\n","            'sample_index': i,\n","            'velocity': np.random.uniform(*Config.VELOCITY_RANGE),\n","            'snr_db': np.random.choice(Config.SNR_DB_RANGE)\n","        }\n","\n","        # Transmitted symbols\n","        for ue in range(Config.NUM_UE):\n","            sym = results['transmitted_symbols'][i][ue]\n","            row[f'symbol_ue{ue}_real'] = np.real(sym)\n","            row[f'symbol_ue{ue}_imag'] = np.imag(sym)\n","\n","        # Precoder data (first 10 antennas for CSV)\n","        precoder = results['precoders_1bit'][i]\n","        for ant in range(min(10, Config.NUM_BS_ANTENNAS)):\n","            row[f'precoder_ant{ant}_real'] = np.real(precoder[ant])\n","            row[f'precoder_ant{ant}_imag'] = np.imag(precoder[ant])\n","\n","        # Received signal (first 5 samples)\n","        rx_signal = results['received_signals'][i]\n","        for t in range(min(5, len(rx_signal))):\n","            row[f'rx_time{t}_real'] = np.real(rx_signal[t])\n","            row[f'rx_time{t}_imag'] = np.imag(rx_signal[t])\n","\n","        data.append(row)\n","\n","    df = pd.DataFrame(data)\n","    df.to_csv(filename, index=False)\n","    print(f\"Saved CSV data to {filename} (shape: {df.shape})\")\n","\n","def run_simulation():\n","    channel_model = ThreeGPPChannel()\n","    results = {\n","        'channels': [],\n","        'precoders_1bit': [],\n","        'received_signals': [],\n","        'transmitted_symbols': []\n","    }\n","\n","    # Pre-allocate arrays for better performance\n","    results['channels'] = np.zeros((Config.NUM_SAMPLES, Config.NUM_SUBCARRIERS, Config.NUM_UE, Config.NUM_BS_ANTENNAS, 2), dtype=np.complex64)\n","    results['precoders_1bit'] = np.zeros((Config.NUM_SAMPLES, Config.NUM_BS_ANTENNAS), dtype=np.complex64)\n","    results['received_signals'] = np.zeros((Config.NUM_SAMPLES, Config.NUM_SUBCARRIERS + Config.CP_LENGTH, Config.NUM_BS_ANTENNAS), dtype=np.complex64)\n","    results['transmitted_symbols'] = np.zeros((Config.NUM_SAMPLES, Config.NUM_UE), dtype=np.complex64)\n","\n","    for t in tqdm(range(Config.NUM_SAMPLES), desc=\"Generating samples\"):\n","        # Channel generation\n","        velocity = np.random.uniform(*Config.VELOCITY_RANGE)\n","        H_true = channel_model.generate(t/1e3, velocity)\n","\n","        # Symbol generation\n","        symbols = generate_symbols()\n","\n","        # 1-bit precoding with proper dimension handling\n","        H_center = H_true[Config.NUM_SUBCARRIERS//2]\n","        if len(H_center.shape) == 3:  # If polarization dimension exists\n","            H_center = H_center[..., 0]  # Take first polarization\n","        if H_center.shape[0] == Config.NUM_BS_ANTENNAS:\n","            H_center = H_center.T\n","        W_1bit = mmse_1bit_precoder(H_center, symbols)\n","\n","        # OFDM transmission\n","        x_freq = np.zeros((Config.NUM_SUBCARRIERS, Config.NUM_BS_ANTENNAS), dtype=complex)\n","        x_freq[Config.NUM_SUBCARRIERS//2] = W_1bit.T\n","        x_time = OFDM.modulate(x_freq)\n","\n","        # AWGN channel\n","        noise_var = 10**(-np.random.choice(Config.SNR_DB_RANGE)/10)\n","        y_time = x_time + np.sqrt(noise_var/2) * (\n","            np.random.randn(*x_time.shape) + 1j*np.random.randn(*x_time.shape))\n","\n","        # Store results\n","        results['channels'][t] = H_true\n","        results['precoders_1bit'][t] = W_1bit\n","        results['received_signals'][t] = y_time\n","        results['transmitted_symbols'][t] = symbols\n","\n","    # Save to HDF5\n","    h5_filename = 'dataset_1bit.h5'\n","    with h5py.File(h5_filename, 'w') as f:\n","        for key, val in results.items():\n","            f.create_dataset(key, data=np.array(val), compression='gzip')\n","\n","    # Save to CSV\n","    csv_filename = 'dataset_1bit.csv'\n","    save_to_csv(results, csv_filename)\n","\n","    return results\n","\n","if __name__ == \"__main__\":\n","    # Clear GPU cache\n","    torch.cuda.empty_cache()\n","\n","    results = run_simulation()\n","    dataset = MassiveMIMO1BitDataset('dataset_1bit.h5')\n","\n","    print(\"\\n=== Dataset Summary ===\")\n","    print(f\"Total samples: {len(dataset)}\")\n","    print(f\"Channel shape: {dataset[0]['channel'].shape}\")\n","    print(f\"Precoder shape: {dataset[0]['precoder'].shape}\")\n","    print(f\"Received signal shape: {dataset[0]['received_signal'].shape}\")\n","    print(\"\\n=== Validation Checks ===\")\n","    print(f\"1-bit constraint: {np.all(np.unique(np.real(dataset[0]['precoder'].cpu().numpy()) == [-1/np.sqrt(2), 1/np.sqrt(2)]))}\")\n","    print(f\"Avg precoder power: {np.mean(np.abs(dataset[0]['precoder'].cpu().numpy())**2):.4f}\")\n","    print(f\"CSV file created: {os.path.exists('dataset_1bit.csv')}\")"],"metadata":{"id":"KGKkeOQFCz5x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Model"],"metadata":{"id":"TJ5zzPe53Fuy"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","class EfficientDepthwiseConv(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1):\n","        super().__init__()\n","        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3,\n","                                 stride=stride, padding=1, groups=in_channels)\n","        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n","        self.norm = nn.GroupNorm(4, out_channels)\n","        self.act = nn.SiLU()\n","\n","    def forward(self, x):\n","        x = self.depthwise(x)\n","        x = self.pointwise(x)\n","        x = self.norm(x)\n","        return self.act(x)\n","\n","class LightweightAttention(nn.Module):\n","    def __init__(self, channels, reduction_ratio=8):\n","        super().__init__()\n","        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n","        self.fc = nn.Sequential(\n","            nn.Linear(channels, channels // reduction_ratio),\n","            nn.ReLU(),\n","            nn.Linear(channels // reduction_ratio, channels),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        b, c, _, _ = x.size()\n","        y = self.avg_pool(x).view(b, c)\n","        y = self.fc(y).view(b, c, 1, 1)\n","        return x * y\n","\n","class OptimizedResBlock(nn.Module):\n","    def __init__(self, in_ch, out_ch):\n","        super().__init__()\n","        self.conv1 = EfficientDepthwiseConv(in_ch, out_ch)\n","        self.attn = LightweightAttention(out_ch)\n","        self.conv2 = EfficientDepthwiseConv(out_ch, out_ch)\n","\n","        self.shortcut = nn.Sequential()\n","        if in_ch != out_ch:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=1, bias=False),\n","                nn.GroupNorm(4, out_ch)\n","            )\n","\n","    def forward(self, x):\n","        residual = self.shortcut(x)\n","        x = self.conv1(x)\n","        x = self.attn(x)\n","        x = self.conv2(x)\n","        return x + residual\n","\n","class PowerAwareQuantizer(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.beta = nn.Parameter(torch.tensor(1.0))\n","        self.sqrt2 = np.sqrt(2)\n","\n","    def forward(self, x):\n","        x_scaled = x / (self.beta * self.sqrt2)\n","        x_clipped = torch.clamp(x_scaled, -1, 1)\n","        quantized = torch.sign(x_clipped) / self.sqrt2\n","        # Power normalization to ensure E[|x|²] = 1\n","        return quantized * torch.sqrt(torch.tensor(2.0))  # Corrects power to 1.0\n","\n","\n","class UltraEfficientPrecoder(nn.Module):\n","    def __init__(self, num_subcarriers=64, num_bs_antennas=32):\n","        super().__init__()\n","        self.M = num_bs_antennas\n","\n","        # Input processing for complex channels\n","        # Expects input shape: [batch, 4, num_subcarriers, num_bs_antennas, 2]\n","        self.input_processor = nn.Sequential(\n","            nn.Conv2d(4*2, 32, kernel_size=3, padding=1),  # Process real and imaginary parts\n","            nn.GroupNorm(4, 32),\n","            nn.SiLU()\n","        )\n","\n","        # Downsampling\n","        self.conv2 = EfficientDepthwiseConv(32, 64, stride=2)\n","\n","        # Residual blocks\n","        self.block1 = OptimizedResBlock(64, 64)\n","        self.block2 = OptimizedResBlock(64, 64)\n","\n","        # Calculate flattened size\n","        with torch.no_grad():\n","            dummy = torch.zeros(1, 8, num_subcarriers, num_bs_antennas)  # 4 users * 2 (real+imag)\n","            dummy = self.input_processor(dummy)\n","            dummy = self.conv2(dummy)\n","            dummy = self.block1(dummy)\n","            dummy = self.block2(dummy)\n","            self.flattened_size = dummy.numel() // dummy.shape[0]\n","\n","        # Output layers\n","        self.fc = nn.Linear(self.flattened_size, 2*self.M)  # 2* for real/imag output if needed\n","        self.quant = PowerAwareQuantizer()\n","\n","    def forward(self, x):\n","        # x shape: [batch, 4, num_subcarriers, num_bs_antennas, 2]\n","        batch_size = x.size(0)\n","\n","        # Process complex inputs\n","        # Reshape to [batch, 4*2, num_subcarriers, num_bs_antennas]\n","        x = x.permute(0, 1, 4, 2, 3).reshape(batch_size, 8, x.size(2), x.size(3))\n","\n","        x = self.input_processor(x)\n","        x = self.conv2(x)\n","        x = self.block1(x)\n","        x = self.block2(x)\n","\n","        # Flatten and output\n","        x = x.reshape(batch_size, -1)\n","        x = self.fc(x)\n","        return self.quant(x)\n","\n","# Verification Test\n","if __name__ == \"__main__\":\n","    # Config\n","    num_subcarriers = 64\n","    num_bs_antennas = 32\n","    batch_size = 4\n","\n","    # Correct complex input tensor [batch, 4, num_subcarriers, num_bs_antennas, 2]\n","    x = torch.randn(batch_size, 4, num_subcarriers, num_bs_antennas, 2)\n","\n","    # Initialize model\n","    model = UltraEfficientPrecoder(num_subcarriers, num_bs_antennas)\n","\n","    # Forward pass\n","    with torch.no_grad():\n","        out = model(x)\n","\n","    print(\"\\n=== Verification ===\")\n","    print(f\"Input shape: {x.shape}\")\n","    print(f\"Output shape: {out.shape}\")\n","    print(f\"Quantization levels: {torch.unique(out)}\")\n","    print(f\"Output power: {torch.mean(torch.abs(out)**2):.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y-g6KayhF8T6","outputId":"4f7c4986-11b6-4477-e7eb-902207aecec3","executionInfo":{"status":"ok","timestamp":1748320408042,"user_tz":-330,"elapsed":4170,"user":{"displayName":"Mathi","userId":"02184997273667573842"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Verification ===\n","Input shape: torch.Size([4, 4, 64, 32, 2])\n","Output shape: torch.Size([4, 64])\n","Quantization levels: tensor([-1.0000,  1.0000])\n","Output power: 1.0000\n"]}]},{"cell_type":"markdown","source":["##Parameter Check"],"metadata":{"id":"bRYgNqOaF9Bb"}},{"cell_type":"code","source":["from thop import profile\n","from thop import clever_format\n","\n","macs, params = profile(model, inputs=(x, ))\n","\n","macs, params = clever_format([macs, params], \"%.3f\")\n","print(f\"GFLOPs: {macs}, Params: {params}\")"],"metadata":{"id":"2sIIZB9R3FCv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9160cbda-2af5-4074-e97a-d31e69abc5cf","executionInfo":{"status":"ok","timestamp":1746870173221,"user_tz":-330,"elapsed":158,"user":{"displayName":"Mathi","userId":"02184997273667573842"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n","[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n","[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n","[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n","GFLOPs: 70.591M, Params: 2.123M\n"]}]},{"cell_type":"markdown","source":["##Dataset Inspection"],"metadata":{"id":"Aq-Kya6BeJE7"}},{"cell_type":"code","source":["import h5py\n","def inspect_h5_file(filepath):\n","    \"\"\"Prints the structure of an HDF5 file\"\"\"\n","    with h5py.File(filepath, 'r') as hf:\n","        print(f\"\\nStructure of {filepath}:\")\n","        def print_attrs(name, obj):\n","            print(f\"  {name}: shape={obj.shape if hasattr(obj, 'shape') else 'group'}\")\n","        hf.visititems(print_attrs)\n","\n","# Run this on one of your files to see the actual structure\n","inspect_h5_file('/content/Final_Year_Project/Final_Year_Project/Dataset/train/h5/dataset_1bit_1.h5')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YEr0rSkARMEG","outputId":"eb139d93-b14f-49bf-ba27-abe81ad4bffd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Structure of /content/Final_Year_Project/Final_Year_Project/Dataset/train/h5/dataset_1bit_1.h5:\n","  channels: shape=(40000, 64, 4, 32, 2)\n","  precoders_1bit: shape=(40000, 32)\n","  received_signals: shape=(40000, 80, 32)\n","  transmitted_symbols: shape=(40000, 4)\n"]}]},{"cell_type":"markdown","source":["##CSV Data Inspection"],"metadata":{"id":"AFelmx53OtIo"}},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_csv('/content/drive/MyDrive/Final_Year_Project/csv/dataset_1bit(1).csv')\n","print(\"Columns:\", df.columns.tolist())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y6Sy5YinwC6V","outputId":"b9234544-d131-413c-dfbe-52160f6044c9","executionInfo":{"status":"ok","timestamp":1746870184744,"user_tz":-330,"elapsed":6814,"user":{"displayName":"Mathi","userId":"02184997273667573842"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Columns: ['sample_index', 'velocity', 'snr_db', 'symbol_ue0_real', 'symbol_ue0_imag', 'symbol_ue1_real', 'symbol_ue1_imag', 'symbol_ue2_real', 'symbol_ue2_imag', 'symbol_ue3_real', 'symbol_ue3_imag', 'precoder_ant0_real', 'precoder_ant0_imag', 'precoder_ant1_real', 'precoder_ant1_imag', 'precoder_ant2_real', 'precoder_ant2_imag', 'precoder_ant3_real', 'precoder_ant3_imag', 'precoder_ant4_real', 'precoder_ant4_imag', 'precoder_ant5_real', 'precoder_ant5_imag', 'precoder_ant6_real', 'precoder_ant6_imag', 'precoder_ant7_real', 'precoder_ant7_imag', 'precoder_ant8_real', 'precoder_ant8_imag', 'precoder_ant9_real', 'precoder_ant9_imag', 'rx_time0_real', 'rx_time0_imag', 'rx_time1_real', 'rx_time1_imag', 'rx_time2_real', 'rx_time2_imag', 'rx_time3_real', 'rx_time3_imag', 'rx_time4_real', 'rx_time4_imag']\n"]}]},{"cell_type":"code","source":["expected = []\n","for i in range(4):\n","    expected.extend([f'symbol_ue{i}_real', f'symbol_ue{i}_imag'])\n","for i in range(10):\n","    expected.extend([f'precoder_ant{i}_real', f'precoder_ant{i}_imag'])\n","print(\"Expected:\", expected)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9qOSobi_wJgJ","outputId":"71c80f71-f74e-4720-e661-0fd1cbf48a21","executionInfo":{"status":"ok","timestamp":1746870184971,"user_tz":-330,"elapsed":9,"user":{"displayName":"Mathi","userId":"02184997273667573842"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Expected: ['symbol_ue0_real', 'symbol_ue0_imag', 'symbol_ue1_real', 'symbol_ue1_imag', 'symbol_ue2_real', 'symbol_ue2_imag', 'symbol_ue3_real', 'symbol_ue3_imag', 'precoder_ant0_real', 'precoder_ant0_imag', 'precoder_ant1_real', 'precoder_ant1_imag', 'precoder_ant2_real', 'precoder_ant2_imag', 'precoder_ant3_real', 'precoder_ant3_imag', 'precoder_ant4_real', 'precoder_ant4_imag', 'precoder_ant5_real', 'precoder_ant5_imag', 'precoder_ant6_real', 'precoder_ant6_imag', 'precoder_ant7_real', 'precoder_ant7_imag', 'precoder_ant8_real', 'precoder_ant8_imag', 'precoder_ant9_real', 'precoder_ant9_imag']\n"]}]},{"cell_type":"markdown","source":["##Training and Testing"],"metadata":{"id":"z2WT1F3MiOkW"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import gc\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","from scipy.spatial.distance import cosine\n","import math\n","\n","class ChunkedCSVDataset(Dataset):\n","    \"\"\"Processes CSV in chunks to save memory\"\"\"\n","    def __init__(self, csv_paths, num_subcarriers=64, num_antennas=32, chunk_size=5000):\n","        self.csv_paths = csv_paths\n","        self.num_subcarriers = num_subcarriers\n","        self.num_antennas = num_antennas\n","        self.chunk_size = chunk_size\n","        self.samples = []\n","\n","        # First pass to verify columns\n","        sample_df = pd.read_csv(csv_paths[0], nrows=1)\n","        self._verify_columns(sample_df.columns)\n","\n","        # Process each file in chunks\n","        for path in csv_paths:\n","            chunk_reader = pd.read_csv(path, chunksize=self.chunk_size)\n","            for chunk in chunk_reader:\n","                self._process_chunk(chunk)\n","\n","    def _verify_columns(self, columns):\n","        required = []\n","        for i in range(4):\n","            required.extend([f'symbol_ue{i}_real', f'symbol_ue{i}_imag'])\n","        for i in range(10):\n","            required.extend([f'precoder_ant{i}_real', f'precoder_ant{i}_imag'])\n","\n","        missing = set(required) - set(columns)\n","        if missing:\n","            raise ValueError(f\"Missing columns: {missing}\")\n","\n","    def _process_chunk(self, chunk):\n","        # Process UE symbols\n","        ue_symbols = np.stack([\n","            chunk[[f'symbol_ue{i}_real', f'symbol_ue{i}_imag']].values\n","            for i in range(4)\n","        ], axis=1)\n","\n","        # Create channel data [chunk_size, 4, 64, 32, 2]\n","        channels = np.zeros((len(chunk), 4, self.num_subcarriers, self.num_antennas, 2))\n","        channels[..., 0] = ue_symbols[:, :, 0][:, :, np.newaxis, np.newaxis]\n","        channels[..., 1] = ue_symbols[:, :, 1][:, :, np.newaxis, np.newaxis]\n","\n","        # Process precoders\n","        precoders = np.zeros((len(chunk), 20))\n","        for i in range(10):\n","            precoders[:, 2*i] = chunk[f'precoder_ant{i}_real'].values\n","            precoders[:, 2*i+1] = chunk[f'precoder_ant{i}_imag'].values\n","\n","        self.samples.extend(list(zip(channels, precoders)))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        channel, precoder = self.samples[idx]\n","        return (\n","            torch.as_tensor(channel, dtype=torch.float32),\n","            torch.as_tensor(precoder, dtype=torch.float32)\n","        )\n","\n","def calculate_snr(predicted, target):\n","    \"\"\"Calculate Signal-to-Noise Ratio (SNR) in dB\"\"\"\n","    noise = predicted - target\n","    signal_power = torch.mean(torch.square(target))\n","    noise_power = torch.mean(torch.square(noise))\n","    snr = 10 * torch.log10(signal_power / noise_power)\n","    return snr.item()\n","\n","def calculate_ber(predicted, target):\n","    \"\"\"Calculate Bit Error Rate (BER) for 1-bit precoding\"\"\"\n","    predicted_bits = (predicted > 0).float()\n","    target_bits = (target > 0).float()\n","    errors = torch.sum(torch.abs(predicted_bits - target_bits))\n","    total_bits = target_bits.numel()\n","    ber = errors / total_bits\n","    return ber.item()\n","\n","def calculate_cosine_similarity(predicted, target):\n","    \"\"\"Calculate Cosine Similarity between predicted and target\"\"\"\n","    predicted = predicted.flatten().detach().cpu().numpy()\n","    target = target.flatten().detach().cpu().numpy()\n","    return 1 - cosine(predicted, target)\n","\n","def train_model():\n","    # Configuration\n","    config = {\n","        'num_subcarriers': 64,\n","        'num_bs_antennas': 32,\n","        'batch_size': 16,  # Increased from 8\n","        'num_epochs': 40,  # Changed to 40 epochs\n","        'learning_rate': 3e-4,\n","        'save_dir': r'/content/drive/MyDrive/Final_year_Project/csv/precoder_checkpoints',\n","        'csv_files': [\n","            r'/content/drive/MyDrive/Final_year_Project/csv/dataset_1bit(1).csv',\n","            r'/content/drive/MyDrive/Final_year_Project/csv/dataset_1bit(1).csv'\n","        ],\n","        'chunk_size': 1000,\n","        'checkpoint_freq': 5,\n","        'lr_scheduler': {\n","            'mode': 'min',\n","            'factor': 0.5,\n","            'patience': 3,\n","            'threshold': 0.0001\n","        }\n","    }\n","\n","    # Setup\n","    os.makedirs(config['save_dir'], exist_ok=True)\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    print(f\"Using device: {device}\")\n","\n","    # Initialize enhanced model\n","    model = UltraEfficientPrecoder(\n","        num_subcarriers=config['num_subcarriers'],\n","        num_bs_antennas=config['num_bs_antennas']\n","    ).to(device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'])\n","    criterion = nn.MSELoss()\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","        optimizer,\n","        mode=config['lr_scheduler']['mode'],\n","        factor=config['lr_scheduler']['factor'],\n","        patience=config['lr_scheduler']['patience'],\n","        threshold=config['lr_scheduler']['threshold']\n","    )\n","\n","    # Track metrics\n","    metrics = {\n","        'train': {'loss': [], 'snr': [], 'ber': [], 'cosine': []},\n","        'val': {'loss': [], 'snr': [], 'ber': [], 'cosine': []},\n","        'test': {'loss': 0, 'snr': 0, 'ber': 0, 'cosine': 0}\n","    }\n","\n","    try:\n","        # Load dataset\n","        dataset = ChunkedCSVDataset(\n","            config['csv_files'],\n","            num_subcarriers=config['num_subcarriers'],\n","            num_antennas=config['num_bs_antennas'],\n","            chunk_size=config['chunk_size']\n","        )\n","\n","        # Split dataset\n","        train_size = int(0.8 * len(dataset))\n","        val_size = int(0.1 * len(dataset))\n","        test_size = len(dataset) - train_size - val_size\n","\n","        train_dataset, val_dataset, test_dataset = random_split(\n","            dataset,\n","            [train_size, val_size, test_size],\n","            generator=torch.Generator().manual_seed(42)\n","        )\n","\n","        print(f\"Dataset sizes - Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n","\n","        # DataLoaders with increased workers\n","        train_loader = DataLoader(\n","            train_dataset,\n","            batch_size=config['batch_size'],\n","            shuffle=True,\n","            num_workers=4,\n","            pin_memory=True\n","        )\n","\n","        val_loader = DataLoader(\n","            val_dataset,\n","            batch_size=config['batch_size'],\n","            shuffle=False,\n","            num_workers=2,\n","            pin_memory=True\n","        )\n","\n","        test_loader = DataLoader(\n","            test_dataset,\n","            batch_size=config['batch_size'],\n","            shuffle=False,\n","            num_workers=2,\n","            pin_memory=True\n","        )\n","\n","        # Training loop for 40 epochs\n","        best_val_loss = float('inf')\n","        for epoch in range(config['num_epochs']):\n","            model.train()\n","            epoch_train_metrics = {'loss': 0, 'snr': 0, 'ber': 0, 'cosine': 0}\n","\n","            # Training phase with gradient clipping\n","            for inputs, targets in tqdm(train_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]} [Train]'):\n","                inputs = inputs.to(device, non_blocking=True)\n","                targets = targets.to(device, non_blocking=True)\n","\n","                # Forward pass\n","                outputs = model(inputs)\n","\n","                # Handle dimension mismatch\n","                if outputs.size(-1) > targets.size(-1):\n","                    targets = torch.cat([\n","                        targets,\n","                        torch.zeros(targets.size(0), outputs.size(-1) - targets.size(-1),\n","                        device=device)\n","                    ], dim=-1)\n","\n","                loss = criterion(outputs, targets)\n","\n","                # Calculate metrics\n","                epoch_train_metrics['loss'] += loss.item()\n","                epoch_train_metrics['snr'] += calculate_snr(outputs, targets)\n","                epoch_train_metrics['ber'] += calculate_ber(outputs, targets)\n","                epoch_train_metrics['cosine'] += calculate_cosine_similarity(outputs, targets)\n","\n","                # Backward pass with gradient clipping\n","                optimizer.zero_grad()\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","                optimizer.step()\n","\n","            # Average training metrics\n","            for k in epoch_train_metrics:\n","                epoch_train_metrics[k] /= len(train_loader)\n","            metrics['train']['loss'].append(epoch_train_metrics['loss'])\n","            metrics['train']['snr'].append(epoch_train_metrics['snr'])\n","            metrics['train']['ber'].append(epoch_train_metrics['ber'])\n","            metrics['train']['cosine'].append(epoch_train_metrics['cosine'])\n","\n","            # Validation phase\n","            model.eval()\n","            epoch_val_metrics = {'loss': 0, 'snr': 0, 'ber': 0, 'cosine': 0}\n","            with torch.no_grad():\n","                for inputs, targets in tqdm(val_loader, desc=f'Epoch {epoch+1}/{config[\"num_epochs\"]} [Val]'):\n","                    inputs = inputs.to(device, non_blocking=True)\n","                    targets = targets.to(device, non_blocking=True)\n","\n","                    outputs = model(inputs)\n","                    if outputs.size(-1) > targets.size(-1):\n","                        targets = torch.cat([\n","                            targets,\n","                            torch.zeros(targets.size(0), outputs.size(-1) - targets.size(-1),\n","                            device=device)\n","                        ], dim=-1)\n","\n","                    loss = criterion(outputs, targets)\n","\n","                    epoch_val_metrics['loss'] += loss.item()\n","                    epoch_val_metrics['snr'] += calculate_snr(outputs, targets)\n","                    epoch_val_metrics['ber'] += calculate_ber(outputs, targets)\n","                    epoch_val_metrics['cosine'] += calculate_cosine_similarity(outputs, targets)\n","\n","            # Average validation metrics and update LR\n","            for k in epoch_val_metrics:\n","                epoch_val_metrics[k] /= len(val_loader)\n","            metrics['val']['loss'].append(epoch_val_metrics['loss'])\n","            metrics['val']['snr'].append(epoch_val_metrics['snr'])\n","            metrics['val']['ber'].append(epoch_val_metrics['ber'])\n","            metrics['val']['cosine'].append(epoch_val_metrics['cosine'])\n","            scheduler.step(epoch_val_metrics['loss'])\n","\n","            print(f\"\\nEpoch {epoch+1}:\")\n","            print(f\"Train - Loss: {epoch_train_metrics['loss']:.4f}, SNR: {epoch_train_metrics['snr']:.2f} dB, BER: {epoch_train_metrics['ber']:.4f}, Cosine: {epoch_train_metrics['cosine']:.4f}\")\n","            print(f\"Val   - Loss: {epoch_val_metrics['loss']:.4f}, SNR: {epoch_val_metrics['snr']:.2f} dB, BER: {epoch_val_metrics['ber']:.4f}, Cosine: {epoch_val_metrics['cosine']:.4f}\")\n","            print(f\"Current LR: {optimizer.param_groups[0]['lr']:.2e}\")\n","\n","            # Save best model\n","            if epoch_val_metrics['loss'] < best_val_loss:\n","                best_val_loss = epoch_val_metrics['loss']\n","                checkpoint = {\n","                    'epoch': epoch + 1,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'metrics': metrics,\n","                    'config': config\n","                }\n","                torch.save(checkpoint, os.path.join(config['save_dir'], 'best_model.pth'))\n","                print(\"Saved new best model\")\n","\n","            # Save periodic checkpoint\n","            if (epoch + 1) % config['checkpoint_freq'] == 0:\n","                torch.save(checkpoint, os.path.join(config['save_dir'], f'checkpoint_epoch_{epoch+1}.pth'))\n","\n","        # Test evaluation\n","        print(\"\\nEvaluating on test set...\")\n","        model.load_state_dict(torch.load(os.path.join(config['save_dir'], 'best_model.pth'))['model_state_dict'])\n","        model.eval()\n","        test_metrics = {'loss': 0, 'snr': 0, 'ber': 0, 'cosine': 0}\n","        with torch.no_grad():\n","            for inputs, targets in tqdm(test_loader, desc=\"Test Evaluation\"):\n","                inputs = inputs.to(device, non_blocking=True)\n","                targets = targets.to(device, non_blocking=True)\n","\n","                outputs = model(inputs)\n","                if outputs.size(-1) > targets.size(-1):\n","                    targets = torch.cat([\n","                        targets,\n","                        torch.zeros(targets.size(0), outputs.size(-1) - targets.size(-1),\n","                        device=device)\n","                    ], dim=-1)\n","\n","                test_metrics['loss'] += criterion(outputs, targets).item()\n","                test_metrics['snr'] += calculate_snr(outputs, targets)\n","                test_metrics['ber'] += calculate_ber(outputs, targets)\n","                test_metrics['cosine'] += calculate_cosine_similarity(outputs, targets)\n","\n","        # Average test metrics\n","        for k in test_metrics:\n","            test_metrics[k] /= len(test_loader)\n","        metrics['test'] = test_metrics\n","\n","        print(\"\\nFinal Test Metrics:\")\n","        print(f\"Loss: {test_metrics['loss']:.4f}, SNR: {test_metrics['snr']:.2f} dB, BER: {test_metrics['ber']:.4f}, Cosine: {test_metrics['cosine']:.4f}\")\n","\n","        # Save final results and plots\n","        results = {\n","            'metrics': metrics,\n","            'config': config,\n","            'best_epoch': checkpoint['epoch']\n","        }\n","        torch.save(results, os.path.join(config['save_dir'], 'final_results.pth'))\n","\n","        # Plot metrics\n","        plt.figure(figsize=(15, 10))\n","\n","        plt.subplot(2, 2, 1)\n","        plt.plot(metrics['train']['loss'], label='Train')\n","        plt.plot(metrics['val']['loss'], label='Validation')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Loss')\n","        plt.legend()\n","\n","        plt.subplot(2, 2, 2)\n","        plt.plot(metrics['train']['snr'], label='Train')\n","        plt.plot(metrics['val']['snr'], label='Validation')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('SNR (dB)')\n","        plt.legend()\n","\n","        plt.subplot(2, 2, 3)\n","        plt.plot(metrics['train']['ber'], label='Train')\n","        plt.plot(metrics['val']['ber'], label='Validation')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('BER')\n","        plt.legend()\n","\n","        plt.subplot(2, 2, 4)\n","        plt.plot(metrics['train']['cosine'], label='Train')\n","        plt.plot(metrics['val']['cosine'], label='Validation')\n","        plt.xlabel('Epoch')\n","        plt.ylabel('Cosine Similarity')\n","        plt.legend()\n","\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(config['save_dir'], 'training_metrics.png'))\n","        plt.close()\n","\n","        print(\"Training completed and results saved!\")\n","\n","    except Exception as e:\n","        print(f\"Error during training: {str(e)}\")\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","\n","if __name__ == '__main__':\n","    train_model()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uDAkSqkk1phX","outputId":"e002a048-bfe9-413a-c07d-6f5626bc4644"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","Dataset sizes - Train: 64000, Val: 8000, Test: 8000\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50 [Train]: 100%|██████████| 8000/8000 [25:07<00:00,  5.31it/s]\n","Epoch 1/50 [Val]: 100%|██████████| 1000/1000 [00:05<00:00, 170.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 1.1558, Val Loss = 1.1550\n","Saved new best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50 [Train]: 100%|██████████| 8000/8000 [25:00<00:00,  5.33it/s]\n","Epoch 2/50 [Val]: 100%|██████████| 1000/1000 [00:06<00:00, 145.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss = 1.1556, Val Loss = 1.1546\n","Saved new best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50 [Train]: 100%|██████████| 8000/8000 [25:01<00:00,  5.33it/s]\n","Epoch 3/50 [Val]: 100%|██████████| 1000/1000 [00:06<00:00, 145.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss = 1.1555, Val Loss = 1.1546\n","Saved new best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50 [Train]: 100%|██████████| 8000/8000 [25:22<00:00,  5.25it/s]\n","Epoch 4/50 [Val]: 100%|██████████| 1000/1000 [00:05<00:00, 177.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss = 1.1555, Val Loss = 1.1545\n","Saved new best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50 [Train]: 100%|██████████| 8000/8000 [25:00<00:00,  5.33it/s]\n","Epoch 5/50 [Val]: 100%|██████████| 1000/1000 [00:06<00:00, 163.76it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Train Loss = 1.1556, Val Loss = 1.1544\n","Saved new best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50 [Train]: 100%|██████████| 8000/8000 [24:35<00:00,  5.42it/s]\n","Epoch 6/50 [Val]: 100%|██████████| 1000/1000 [00:06<00:00, 163.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Train Loss = 1.1556, Val Loss = 1.1545\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50 [Train]: 100%|██████████| 8000/8000 [25:19<00:00,  5.26it/s]\n","Epoch 7/50 [Val]: 100%|██████████| 1000/1000 [00:06<00:00, 152.39it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Train Loss = 1.1555, Val Loss = 1.1542\n","Saved new best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50 [Train]: 100%|██████████| 8000/8000 [25:10<00:00,  5.30it/s]\n","Epoch 8/50 [Val]: 100%|██████████| 1000/1000 [00:05<00:00, 172.61it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Train Loss = 1.1557, Val Loss = 1.1541\n","Saved new best model\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50 [Train]: 100%|██████████| 8000/8000 [25:26<00:00,  5.24it/s]\n","Epoch 9/50 [Val]: 100%|██████████| 1000/1000 [00:06<00:00, 157.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Train Loss = 1.1558, Val Loss = 1.1545\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50 [Train]:  81%|████████  | 6460/8000 [20:30<04:37,  5.55it/s]"]}]},{"cell_type":"markdown","source":["Epoch 22: Train Loss = 1.1565, Val Loss = 1.1572\n","Saved new best model\n","Epoch 23/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [41:07<00:00,  3.24it/s]\n","Epoch 23/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [10:33<00:00,  1.58it/s]\n","Epoch 23: Train Loss = 1.1566, Val Loss = 1.1575\n","Epoch 24/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [45:22<00:00,  2.94it/s]\n","Epoch 24/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [20:47<00:00,  1.25s/it]\n","Epoch 24: Train Loss = 1.1564, Val Loss = 1.1576\n","Epoch 25/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [1:20:06<00:00,  1.66it/s]\n","Epoch 25/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [17:34<00:00,  1.05s/it]\n","Epoch 25: Train Loss = 1.1564, Val Loss = 1.1576\n","Epoch 26/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [52:01<00:00,  2.56it/s]\n","Epoch 26/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [14:42<00:00,  1.13it/s]\n","Epoch 26: Train Loss = 1.1564, Val Loss = 1.1575\n","Epoch 27/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [54:16<00:00,  2.46it/s]\n","Epoch 27/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [17:02<00:00,  1.02s/it]\n","Epoch 27: Train Loss = 1.1564, Val Loss = 1.1574\n","Epoch 28/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [49:34<00:00,  2.69it/s]\n","Epoch 28/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [13:17<00:00,  1.25it/s]\n","Epoch 28: Train Loss = 1.1565, Val Loss = 1.1572\n","Saved new best model\n","Epoch 29/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [51:09<00:00,  2.61it/s]\n","Epoch 29/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [19:54<00:00,  1.19s/it]\n","Epoch 29: Train Loss = 1.1566, Val Loss = 1.1573\n","Epoch 30/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [1:02:13<00:00,  2.14it/s]\n","Epoch 30/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [14:29<00:00,  1.15it/s]\n","Epoch 30: Train Loss = 1.1566, Val Loss = 1.1573\n","Epoch 31/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [45:00<00:00,  2.96it/s]\n","Epoch 31/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [11:46<00:00,  1.42it/s]\n","Epoch 31: Train Loss = 1.1566, Val Loss = 1.1572\n","Saved new best model\n","Epoch 32/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [43:49<00:00,  3.04it/s]\n","Epoch 32/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [11:41<00:00,  1.43it/s]\n","Epoch 32: Train Loss = 1.1566, Val Loss = 1.1571\n","Saved new best model\n","Epoch 33/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [44:19<00:00,  3.01it/s]\n","Epoch 33/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [12:09<00:00,  1.37it/s]\n","Epoch 33: Train Loss = 1.1566, Val Loss = 1.1569\n","Saved new best model\n","Epoch 34/50 [Train]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [1:45:32<00:00,  1.26it/s]\n","Epoch 34/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [13:59<00:00,  1.19it/s]\n","Epoch 34: Train Loss = 1.1566, Val Loss = 1.1567\n","Saved new best model\n","Epoch 35/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [50:14<00:00,  2.65it/s]\n","Epoch 35/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [15:35<00:00,  1.07it/s]\n","Epoch 35: Train Loss = 1.1566, Val Loss = 1.1568\n","Epoch 36/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [50:59<00:00,  2.62it/s]\n","Epoch 36/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [18:24<00:00,  1.10s/it]\n","Epoch 36: Train Loss = 1.1565, Val Loss = 1.1568\n","Epoch 37/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [49:05<00:00,  2.72it/s]\n","Epoch 37/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [12:01<00:00,  1.39it/s]\n","Epoch 37: Train Loss = 1.1565, Val Loss = 1.1570\n","Epoch 38/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [49:43<00:00,  2.68it/s]\n","Epoch 38/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [14:39<00:00,  1.14it/s]\n","Epoch 38: Train Loss = 1.1566, Val Loss = 1.1569\n","Epoch 39/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [45:42<00:00,  2.92it/s]\n","Epoch 39/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [12:58<00:00,  1.28it/s]\n","Epoch 39: Train Loss = 1.1566, Val Loss = 1.1569\n","Epoch 40/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [50:13<00:00,  2.65it/s]\n","Epoch 40/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [13:00<00:00,  1.28it/s]\n","Epoch 40: Train Loss = 1.1566, Val Loss = 1.1570\n","Epoch 41/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [46:11<00:00,  2.89it/s]\n","Epoch 41/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [14:36<00:00,  1.14it/s]\n","Epoch 41: Train Loss = 1.1566, Val Loss = 1.1569\n","Epoch 42/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [48:34<00:00,  2.75it/s]\n","Epoch 42/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [12:39<00:00,  1.32it/s]\n","Epoch 42: Train Loss = 1.1567, Val Loss = 1.1570\n","Epoch 43/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [46:47<00:00,  2.85it/s]\n","Epoch 43/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [18:43<00:00,  1.12s/it]\n","Epoch 43: Train Loss = 1.1568, Val Loss = 1.1572\n","Epoch 44/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [46:20<00:00,  2.88it/s]\n","Epoch 44/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [12:43<00:00,  1.31it/s]\n","Epoch 44: Train Loss = 1.1566, Val Loss = 1.1573\n","Epoch 45/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [50:12<00:00,  2.66it/s]\n","Epoch 45/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [16:50<00:00,  1.01s/it]\n","Epoch 45: Train Loss = 1.1567, Val Loss = 1.1578\n","Epoch 46/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [48:08<00:00,  2.77it/s]\n","Epoch 46/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [13:17<00:00,  1.25it/s]\n","Epoch 46: Train Loss = 1.1567, Val Loss = 1.1579\n","Epoch 47/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [52:01<00:00,  2.56it/s]\n","Epoch 47/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [16:20<00:00,  1.02it/s]\n","Epoch 47: Train Loss = 1.1565, Val Loss = 1.1578\n","Epoch 48/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [49:04<00:00,  2.72it/s]\n","Epoch 48/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [18:22<00:00,  1.10s/it]\n","Epoch 48: Train Loss = 1.1564, Val Loss = 1.1579\n","Epoch 49/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [47:53<00:00,  2.78it/s]\n","Epoch 49/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [13:20<00:00,  1.25it/s]\n","Epoch 49: Train Loss = 1.1564, Val Loss = 1.1582\n","Epoch 50/50 [Train]: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 8000/8000 [49:10<00:00,  2.71it/s]\n","Epoch 50/50 [Val]: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [17:17<00:00,  1.04s/it]\n","Epoch 50: Train Loss = 1.1563, Val Loss = 1.1571, snr = 18.7, ber = 4.6, cosine = 0.91\n","\n","Evaluating on test set...\n","Test Evaluation: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1000/1000 [15:07<00:00,  1.10it/s]\n","\n","Final Test Loss: 1.1539, snr = 18.2, ber = 5.0, cosine = 0.8634"],"metadata":{"id":"QgM59yl3BuE4"}}]}